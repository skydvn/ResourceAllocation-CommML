{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd '/content/drive/Shareddrives/Duong-TungPNU/Auto-encoder'\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdyiuRlGqDKI",
        "outputId": "878103b1-707c-41cd-a83b-aec7a59ad813"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/Shareddrives/Duong-TungPNU/Auto-encoder\n",
            "Autoencoder-CommData.ipynb  data  Multihead-DNN-CommData.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_VMxpBesnnFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5acc014-09fd-40aa-8e81-38dc9ff89275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15, 36480)\n",
            "(5, 14592)\n",
            "(15, 14592)\n",
            "(15, 7296)\n",
            "(15, 7296)\n",
            "7296\n",
            "CorAP: [[518.6   238.49  347.61  250.7   554.56 ]\n",
            " [392.47  172.25  338.51  499.71   54.133]\n",
            " [  0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.   ]]\n",
            "CorID: [[134.57  140.04  557.65  231.31  351.75 ]\n",
            " [385.62  279.99  134.63  351.59  213.77 ]\n",
            " [ 83.338 299.32  540.63  393.69  217.6  ]\n",
            " [540.76  453.15  165.98  294.28  522.73 ]\n",
            " [448.59  167.98  189.45  354.42  371.36 ]\n",
            " [  0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.   ]]\n",
            "MU: [2. 2. 2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "#@title Read data from .csv\n",
        "\n",
        "# Path\n",
        "channel_path = \"./data/ChannelAll.csv\"\n",
        "CorAPs_path = \"./data/CorAPsAll.csv\"\n",
        "CorIDs_path = \"./data/CorIDsAll.csv\"\n",
        "power_path = \"./data/Power.csv\"\n",
        "mu_path = \"./data/Mu.csv\"\n",
        "\n",
        "# Read csv\n",
        "channel_array = np.genfromtxt(channel_path, delimiter=',')\n",
        "CorAPs_array = np.genfromtxt(CorAPs_path, delimiter=',')\n",
        "CorIDs_array = np.genfromtxt(CorIDs_path, delimiter=',')\n",
        "power_array = np.genfromtxt(power_path, delimiter=',')\n",
        "mu_array = np.genfromtxt(mu_path, delimiter=',')\n",
        "\n",
        "print(np.shape(channel_array))\n",
        "print(np.shape(CorAPs_array))\n",
        "print(np.shape(CorIDs_array))\n",
        "print(np.shape(power_array))\n",
        "print(np.shape(mu_array))\n",
        "\n",
        "NoSamples=np.shape(power_array)[1]\n",
        "NoIDs_max=15 # maximum number of IDs\n",
        "NoAPs_max=5  # maximum number of APs\n",
        "\n",
        "print(NoSamples)\n",
        "print(f\"CorAP: {CorAPs_array[:,0:5]}\")\n",
        "print(f\"CorID: {CorIDs_array[:,0:5]}\")\n",
        "print(f\"MU: {mu_array[:,0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data pre-processing**\n",
        "- 5000 samples, \n",
        "- 1 sample: \n",
        "            input size 75x1\n",
        "            output size 30x1"
      ],
      "metadata": {
        "id": "PXyJlP8cRrQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inputMode = 1\n",
        "\n",
        "\n",
        "# inputDim = (NoIDs_max+NoAPs_max)*2 # coordinates \n",
        "inputDim = NoIDs_max*NoAPs_max # channel gain \n",
        "outputDim = NoIDs_max*2\n",
        "outputDim1 = NoIDs_max\n",
        "outputDim2 = NoIDs_max\n",
        "\n",
        "# Input coordinates\n",
        "# InputSample=np.zeros((NoSamples,(NoIDs_max+NoAPs_max)*2))\n",
        "# for i in range(NoSamples):\n",
        "#   InputSample[i,0:NoAPs_max]=np.transpose(CorAPs_array[:,i*2])\n",
        "#   InputSample[i,NoAPs_max:NoAPs_max*2]=np.transpose(CorAPs_array[:,i*2+1])\n",
        "#   InputSample[i,NoAPs_max*2:NoAPs_max*2+NoIDs_max]=np.transpose(CorIDs_array[:,i*2])\n",
        "#   InputSample[i,NoAPs_max*2+NoIDs_max:NoAPs_max*2+NoIDs_max*2]=np.transpose(CorIDs_array[:,i*2+1])\n",
        "# print(np.shape(InputSample))\n",
        "# print(InputSample[0,:])\n",
        "# Input channel\n",
        "InputSample=np.zeros((NoSamples,NoIDs_max*NoAPs_max))\n",
        "for i in range(NoSamples):\n",
        "  for j in range(NoAPs_max):\n",
        "    InputSample[i,NoIDs_max*j:NoIDs_max*(j+1)]=np.transpose(channel_array[:,i*NoAPs_max+j])\n",
        "print(InputSample[0,:])\n",
        "# print(np.shape(InputSample))\n",
        "\n",
        "# Output data processing\n",
        "OutputLabel=np.zeros((NoSamples,NoIDs_max*2))\n",
        "for i in range(NoSamples): \n",
        "  OutputLabel[i,0:NoIDs_max]=np.transpose(power_array[:,i])\n",
        "  OutputLabel[i,NoIDs_max:NoIDs_max*2]=np.transpose(mu_array[:,i])\n",
        "\n",
        "print(np.shape(InputSample))\n",
        "print(np.shape(OutputLabel))\n",
        "print(OutputLabel[0:3,NoIDs_max:NoIDs_max*2])\n"
      ],
      "metadata": {
        "id": "A8ghNSNiRpvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86d050f-e05e-450d-968b-724febffc31b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.9824e-06 4.2046e-05 4.5659e-07 1.3320e-05 1.2620e-04 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 4.3381e-06 4.2897e-05 1.2456e-05\n",
            " 3.5508e-06 1.1312e-04 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            "(7296, 75)\n",
            "(7296, 30)\n",
            "[[2. 2. 2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [2. 1. 2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 2. 2. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization**"
      ],
      "metadata": {
        "id": "m7DQOQDj7XsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalized Input\n",
        "InputNormalized=InputSample*0\n",
        "InputLogNormalized=InputSample*0\n",
        "for i in range(inputDim):\n",
        "  term= [j for j in InputSample[:,i] if j > 0] \n",
        "  # [j for j in InputSample[i,:] if j > 0]\n",
        "  if len(term)!=0:\n",
        "    maxTerm=max(term)\n",
        "    minTerm=min(term)\n",
        "    diff=maxTerm-minTerm\n",
        "    for j in range(NoSamples):\n",
        "      if InputSample[j,i]>0:\n",
        "        # InputNormalized[j,i]=(InputSample[j,i]-minTerm)/diff\n",
        "        InputNormalized[j,i]=np.log(InputSample[j,i])\n",
        "      # if InputSample[i,j]==minTerm:\n",
        "      #   InputLogNormalized[i,j]=0\n",
        "      # else:\n",
        "      #   InputLogNormalized[i,j]=np.log(InputNormalized[i,j])\n",
        "\n",
        "print(\"Input Sample\")\n",
        "print(InputSample[0,:])\n",
        "\n",
        "print(\"Input Normalized\")\n",
        "print(InputNormalized[0,:])\n",
        "\n",
        "# print(\"Input Log Normalized\")\n",
        "# print(InputLogNormalized[0,:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-Vxc2y37XZM",
        "outputId": "6ec1d4ee-fbcf-480e-869f-88cfbb183438"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sample\n",
            "[1.9824e-06 4.2046e-05 4.5659e-07 1.3320e-05 1.2620e-04 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 4.3381e-06 4.2897e-05 1.2456e-05\n",
            " 3.5508e-06 1.1312e-04 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            "Input Normalized\n",
            "[-13.13120233 -10.0767463  -14.59948    -11.22624389  -8.97764261\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            " -12.34807409 -10.05670866 -11.29330812 -12.54833763  -9.08706136\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalized Output\n",
        "OutputNormalized=OutputLabel*0\n",
        "OutputLogNormalized=OutputLabel*0\n",
        "\n",
        "for i in range(NoIDs_max):\n",
        "  term= [j for j in OutputLabel[:,i] if j > 0] \n",
        "  if len(term)!=0:\n",
        "    maxTerm=max(term)\n",
        "    minTerm=min(term)\n",
        "    diff=maxTerm-minTerm\n",
        "    for j in range(NoSamples):\n",
        "      OutputNormalized[j,i]=(OutputLabel[j,i]-minTerm)/diff\n",
        "      # OutputNormalized[j,i]=np.log(OutputLabel[j,i])\n",
        "    # if OutputLabel[i,j]==minTerm:\n",
        "    #   OutputLogNormalized[i,j]=0\n",
        "    # else:\n",
        "    #   OutputLogNormalized[i,j]=np.log(OutputNormalized[i,j])\n",
        "print(OutputLabel[0,:])\n",
        "print(OutputNormalized[0,:])\n",
        "# print(OutputLogNormalized[0,:])\n",
        "\n",
        "for i in range(NoIDs_max,NoIDs_max*2):\n",
        "  term= [j for j in OutputLabel[:,i] if j > 0] \n",
        "  if len(term)!=0:\n",
        "    maxTerm=max(term)\n",
        "    minTerm=0\n",
        "    diff=maxTerm-minTerm\n",
        "    for j in range(NoSamples):\n",
        "      OutputNormalized[j,i]=(OutputLabel[j,i]-minTerm)/diff\n",
        "      # OutputNormalized[j,i]=np.log(OutputLabel[j,i])\n",
        "    # if OutputLabel[i,j]==minTerm:\n",
        "    #   OutputLogNormalized[i,j]=0\n",
        "    # else:\n",
        "    #   OutputLogNormalized[i,j]=np.log(OutputNormalized[i,j])\n",
        "print(OutputNormalized[0,:])\n",
        "# print(OutputLogNormalized[0,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4t7yMAkJq4o",
        "outputId": "cb1427aa-379e-4a3a-ed1d-ed0d96085def"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0805e+01 5.4370e-05 1.8734e+00 4.3076e+01 2.5692e+02 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 2.0000e+00 2.0000e+00 2.0000e+00\n",
            " 1.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
            " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            "[3.60166665e-02 1.80158864e-07 7.28807296e-03 1.43586666e-01\n",
            " 9.99494262e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00]\n",
            "[3.60166665e-02 1.80158864e-07 7.28807296e-03 1.43586666e-01\n",
            " 9.99494262e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00 5.00000000e-01 5.00000000e-01\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single-task DNN"
      ],
      "metadata": {
        "id": "PJ9Q_kwSqDhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DNN model\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "class neuralNetwork(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(neuralNetwork, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(in_dim, int((in_dim-out_dim)*4/5+out_dim)),\n",
        "            nn.ReLU(True))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(int((in_dim-out_dim)*4/5+out_dim), int((in_dim-out_dim)*3/5+out_dim)),\n",
        "            nn.ReLU(True))\n",
        "        self.layer15 = nn.Sequential(\n",
        "            nn.Linear(int((in_dim-out_dim)*4/5+out_dim), int((in_dim-out_dim)*4/5+out_dim)),\n",
        "            nn.ReLU(True))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(int((in_dim-out_dim)*3/5+out_dim), int((in_dim-out_dim)*2/5+out_dim)),\n",
        "            nn.ReLU(True))\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Linear(int((in_dim-out_dim)*2/5+out_dim), int((in_dim-out_dim)*1/5+out_dim)),\n",
        "            nn.ReLU(True))                    \n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Linear(int((in_dim-out_dim)*1/5+out_dim), out_dim),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer15(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        return x\n",
        "\n",
        "'''Focal Loss'''\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_loss = F.binary_cross_entropy(inputs.squeeze(),  targets.float())\n",
        "        loss = self.alpha * (1 - torch.exp(-bce_loss)) ** self.gamma * bce_loss\n",
        "        return loss"
      ],
      "metadata": {
        "id": "ltfZudJy7aOd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 1e-5\n",
        "num_epochs = 100\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "model = neuralNetwork(inputDim,outputDim)\n",
        "if use_gpu:\n",
        "    model = model.cuda()\n",
        "\n",
        "criterion = FocalLoss()\n",
        "criterion2 = torch.nn.MSELoss()\n",
        "# criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "### Input sample here ###\n",
        "# InputSample\n",
        "# InputNormalized\n",
        "# InputLogNormalized\n",
        "# OutputLabel\n",
        "# OutputNormalized\n",
        "# OutputLogNormalized\n",
        "tensor_x = torch.Tensor(InputNormalized)\n",
        "\n",
        "tensor_y = torch.Tensor(OutputNormalized)\n",
        "print(tensor_x)\n",
        "joint_dataset = TensorDataset(tensor_x,tensor_y)\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(joint_dataset, [6500, 796])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kk1uZgYXfIb",
        "outputId": "158c1828-b93c-4ab4-f53c-b700216c25fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-13.1312, -10.0767, -14.5995,  ...,   0.0000,   0.0000,   0.0000],\n",
            "        [-12.1082, -11.5676, -11.1121,  ...,   0.0000,   0.0000,   0.0000],\n",
            "        [-10.7105, -12.8105, -11.6129,  ...,   0.0000,   0.0000,   0.0000],\n",
            "        ...,\n",
            "        [-11.3608, -13.3235, -12.7738,  ...,   0.0000,   0.0000,   0.0000],\n",
            "        [-10.7944, -11.8972, -13.3578,  ...,   0.0000,   0.0000,   0.0000],\n",
            "        [-12.0620,  -7.6964, -10.3835,  ...,   0.0000,   0.0000,   0.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LossSave=[]\n",
        "EvalLossSave=[]\n",
        "OutSave=[]\n",
        "for epoch in range(num_epochs): # num_epochs\n",
        "    print('*' * 10)\n",
        "    print(f'epoch {epoch+1}')\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    for i, data in enumerate(train_loader, 1):\n",
        "        img, label = data    \n",
        "        img = img.view(img.size(0), -1)\n",
        "        if use_gpu:\n",
        "            img = img.cuda()\n",
        "            label = label.cuda()\n",
        "        out = model(img) \n",
        "\n",
        "        # estimate out with label\n",
        "        loss = criterion(out, label)\n",
        "        running_loss += loss.item()\n",
        "        # _, pred = torch.max(out, 1)\n",
        "        # print(pred)\n",
        "        # print(label)\n",
        "        # running_acc += (pred == label).float().mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 300 == 0:\n",
        "            print(f'[{epoch+1}/{num_epochs}] Loss: {running_loss/i:.6f}')\n",
        "    print(f'Finish {epoch+1} epoch, Loss: {running_loss/i:.6f}')\n",
        "    LossSave.append(running_loss/i)\n",
        "    model.eval()\n",
        "    eval_loss = 0.\n",
        "    eval_acc = 0.\n",
        "    eval_mse = 0.\n",
        "    for data in test_loader:\n",
        "        img, label = data\n",
        "        img = img.view(img.size(0), -1)\n",
        "        if use_gpu:\n",
        "            img = img.cuda()\n",
        "            label = label.cuda()\n",
        "        with torch.no_grad():\n",
        "            out = model(img)\n",
        "\n",
        "            loss1 = criterion(out, label)\n",
        "            loss2 = criterion2(out, label)\n",
        "        eval_loss += loss1.item()\n",
        "        eval_mse  += loss2.item()\n",
        "        \n",
        "        # if OutSave = None: \n",
        "        #   OutSave = out\n",
        "        # else:\n",
        "        #   OutSave = np.concatenate(OutSave, out, dim = )\n",
        "        # _, pred = torch.max(out, 1)\n",
        "        # eval_acc += (pred == label).float().mean()\n",
        "    # print(f'Test Loss on CE: {eval_loss/len(test_loader):.6f}\\n')\n",
        "    print(f'Test Loss on MSE: {eval_mse/len(test_loader):.6f}\\n')\n",
        "    EvalLossSave.append(eval_loss/len(test_loader))"
      ],
      "metadata": {
        "id": "yVtaL5o9B0L_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "outputId": "b276e3dc-6ae6-4a7e-f050-e6c55baf6094"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********\n",
            "epoch 1\n",
            "Finish 1 epoch, Loss: 0.172402\n",
            "Test Loss on MSE: 0.219164\n",
            "\n",
            "**********\n",
            "epoch 2\n",
            "Finish 2 epoch, Loss: 0.168573\n",
            "Test Loss on MSE: 0.214958\n",
            "\n",
            "**********\n",
            "epoch 3\n",
            "Finish 3 epoch, Loss: 0.159641\n",
            "Test Loss on MSE: 0.202531\n",
            "\n",
            "**********\n",
            "epoch 4\n",
            "Finish 4 epoch, Loss: 0.138077\n",
            "Test Loss on MSE: 0.176206\n",
            "\n",
            "**********\n",
            "epoch 5\n",
            "Finish 5 epoch, Loss: 0.102975\n",
            "Test Loss on MSE: 0.135969\n",
            "\n",
            "**********\n",
            "epoch 6\n",
            "Finish 6 epoch, Loss: 0.064947\n",
            "Test Loss on MSE: 0.095893\n",
            "\n",
            "**********\n",
            "epoch 7\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ead5675781ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# estimate out with label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-573f783cb944>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer15\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor_x[0])\n",
        "print(np.shape(tensor_x[0]))\n",
        "print(\"=======================\")\n",
        "print(f\"in Test: {tensor_y[1]}\")\n",
        "outTest=model(tensor_x[1])\n",
        "print(f\"out Test: {outTest}\")"
      ],
      "metadata": {
        "id": "cB-ZoWuzOG1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f9f785-41fd-468f-a1ad-d8000b29a56d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-13.1312, -10.0767, -14.5995, -11.2262,  -8.9776,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000, -12.3481, -10.0567, -11.2933, -12.5483,  -9.0871,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
            "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000])\n",
            "torch.Size([75])\n",
            "=======================\n",
            "in Test: tensor([1.5217e-04, 2.8979e-05, 4.0821e-05, 3.0912e-05, 1.8476e-05, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 5.0000e-01, 1.0000e+00,\n",
            "        5.0000e-01, 5.0000e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n",
            "out Test: tensor([0.2841, 0.4674, 0.4238, 0.2690, 0.4029, 0.1497, 0.3347, 0.4024, 0.2074,\n",
            "        0.1785, 0.2548, 0.0482, 0.4610, 0.3360, 0.0662, 0.6987, 0.5524, 0.8911,\n",
            "        0.7776, 0.6665, 0.1914, 0.3332, 0.2467, 0.4330, 0.2874, 0.2201, 0.2982,\n",
            "        0.4246, 0.2234, 0.2487], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(3, 2, requires_grad=True)\n",
        "print(input)\n",
        "target = torch.rand(3, 2, requires_grad=False)\n",
        "print(target)\n",
        "loss = F.binary_cross_entropy(torch.sigmoid(input), target)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "HD7q5e2OlNRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1e3eb9-6b3e-44e4-9675-42d6ee16a6e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.6455,  1.0611],\n",
            "        [ 0.4092,  1.1183],\n",
            "        [-0.2332, -1.9834]], requires_grad=True)\n",
            "tensor([[0.5892, 0.2301],\n",
            "        [0.8523, 0.7714],\n",
            "        [0.9192, 0.7170]])\n",
            "tensor(0.9529, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(LossSave)\n",
        "plt.plot(EvalLossSave)"
      ],
      "metadata": {
        "id": "wv3_S4q7gI6_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "c8a22057-6cb5-4c50-df44-4fe4798e763b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcf5ae4fcd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# InputSample\n",
        "# InputNormalized\n",
        "# InputLogNormalized\n",
        "# OutputLabel\n",
        "# OutputNormalized\n",
        "# OutputLogNormalized\n",
        "\n",
        "tensor_x = torch.Tensor(InputNormalized)\n",
        "\n",
        "tensor_y = torch.Tensor(OutputNormalized)\n",
        " \n",
        "print(tensor_x)\n",
        "print(tensor_y)\n",
        "\n",
        "joint_dataset = TensorDataset(tensor_x,tensor_y)\n",
        "# train_dataset, val_dataset = torch.utils.data.random_split(joint_dataset, [9000, 737])\n",
        "\n",
        "train_loader = DataLoader(joint_dataset, batch_size=batch_size, shuffle=False)\n",
        "# test_loader  = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for i, data in enumerate(train_loader, 1):\n",
        "    img, label = data   \n",
        "    img_view = img.view(img.size(0), -1)\n",
        "    if use_gpu:\n",
        "        img = img.cuda()\n",
        "        label = label.cuda()\n",
        "    out = model(img)\n",
        "\n",
        "    if i==1:\n",
        "        print(\"TEST\")\n",
        "        print(\"img\")\n",
        "        print(img_view)\n",
        "        print(label)\n",
        "        print(out)\n",
        "        break\n"
      ],
      "metadata": {
        "id": "t0HbbOjiDM6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-task DNN"
      ],
      "metadata": {
        "id": "3BLc9LMVqE46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "#@title DNN model\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "class neuralNetwork(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim1, out_dim2):\n",
        "        super(neuralNetwork, self).__init__()\n",
        "        self.out_dim = out_dim1 + out_dim2\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(in_dim, int((in_dim-self.out_dim)*4/5+self.out_dim)),\n",
        "            nn.ReLU(True))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(int((in_dim-self.out_dim)*4/5+self.out_dim), int((in_dim-self.out_dim)*3/5+self.out_dim)),\n",
        "            nn.ReLU(True))\n",
        "        self.layer15 = nn.Sequential(\n",
        "            nn.Linear(int((in_dim-self.out_dim)*4/5+self.out_dim), int((in_dim-self.out_dim)*4/5+self.out_dim)),\n",
        "            nn.ReLU(True))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(int((in_dim-self.out_dim)*3/5+self.out_dim), int((in_dim-self.out_dim)*2/5+self.out_dim)),\n",
        "            nn.ReLU(True))\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Linear(int((in_dim-self.out_dim)*2/5+self.out_dim), int((in_dim-self.out_dim)*1/5+self.out_dim)),\n",
        "            nn.ReLU(True))                    \n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Linear(int((in_dim-self.out_dim)*1/5+self.out_dim), self.out_dim),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "        # Create the two output heads\n",
        "        self.head1 = nn.Sequential(\n",
        "            nn.Linear(int(self.out_dim), out_dim1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(out_dim1, out_dim1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.head2 = nn.Sequential(\n",
        "            nn.Linear(int(self.out_dim), out_dim2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(out_dim2, out_dim2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer15(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        out1 = self.head1(x)\n",
        "        out2 = self.head2(x)\n",
        "        return out1, out2\n",
        "\n",
        "'''Focal Loss'''\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_loss = F.binary_cross_entropy(inputs.squeeze(),  targets.float())\n",
        "        loss = self.alpha * (1 - torch.exp(-bce_loss)) ** self.gamma * bce_loss\n",
        "        return loss\n",
        "      "
      ],
      "metadata": {
        "id": "U4vwA-ZjrYwM"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "inputDim = NoIDs_max*NoAPs_max # channel gain \n",
        "outputDim = NoIDs_max*2\n",
        "outputDim1 = NoIDs_max\n",
        "outputDim2 = NoIDs_max\n",
        "\n",
        "print(f\"out1: {outputDim1}, out2: {outputDim2}\")\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the model, criterion, and optimizer\n",
        "batch_size = 32\n",
        "learning_rate = 1e-5\n",
        "num_epochs = 100\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "model = neuralNetwork(inputDim,outputDim1, outputDim2)\n",
        "if use_gpu:\n",
        "    model = model.cuda()\n",
        "\n",
        "criterion1 = FocalLoss() \n",
        "criterion2 = nn.MSELoss() \n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Data Pre-processing \n",
        "i_data=np.zeros((NoSamples,NoIDs_max*NoAPs_max))\n",
        "for i in range(NoSamples):\n",
        "  for j in range(NoAPs_max):\n",
        "    i_data[i,NoIDs_max*j:NoIDs_max*(j+1)]=np.transpose(channel_array[:,i*NoAPs_max+j])\n",
        "i_data  = i_data\n",
        "print(np.shape(i_data))\n",
        "o_task1 = np.transpose(power_array)\n",
        "print(np.shape(o_task1))\n",
        "o_task2 = np.transpose(mu_array)\n",
        "print(np.shape(o_task2))\n",
        "\n",
        "# # Define the input data and targets\n",
        "x  = torch.from_numpy(i_data).to(dtype=torch.float32, device=device)  # input size: (batch_size, 1, sequence_length)\n",
        "y1 = torch.from_numpy(o_task1).to(dtype=torch.float32, device=device)     # output size: (batch_size, num_filters)\n",
        "y2 = torch.from_numpy(o_task2).to(dtype=torch.float32, device=device)     # output size: (batch_size, num_filters)\n",
        "\n",
        "# print(f\"x: {x.size()}, y1: {y1.size()}, y2: {y2.size()}\")\n",
        "\n",
        "tensor_x = torch.Tensor(InputNormalized)\n",
        "\n",
        "tensor_y = torch.Tensor(OutputNormalized)\n",
        "# print(f\"{tensor_x.size()}, {tensor_y.size()}\")\n",
        "joint_dataset = TensorDataset(tensor_x,tensor_y)\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(joint_dataset, [6500, 796])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "LossSave=[]\n",
        "EvalLossSave=[]\n",
        "OutSave=[]\n",
        "for epoch in range(num_epochs): # num_epochs\n",
        "    print('*' * 10)\n",
        "    print(f'epoch {epoch+1}')\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    for i, data in enumerate(train_loader, 1):\n",
        "        img, label = data    \n",
        "        # Split the tensor into two tensors\n",
        "        label_t1, label_t2 = torch.split(label, [15, 15], dim=1)\n",
        "\n",
        "        img = img.view(img.size(0), -1)\n",
        "        if use_gpu:\n",
        "            img = img.cuda()\n",
        "            label = label.cuda()\n",
        "        out1, out2 = model(img) \n",
        "        # estimate out with label\n",
        "        loss1 = criterion1(out1, label_t1)\n",
        "        loss2 = criterion1(out2, label_t2)\n",
        "        loss  = loss1 + loss2\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 300 == 0:\n",
        "            print(f'[{epoch+1}/{num_epochs}] Loss: {running_loss/i:.6f}')\n",
        "    print(f'Finish {epoch+1} epoch, Loss: {running_loss/i:.6f}')\n",
        "    LossSave.append(running_loss/i)\n",
        "    model.eval()\n",
        "    eval_loss = 0.\n",
        "    eval_acc = 0.\n",
        "    eval_mse = 0.\n",
        "    for data in test_loader:\n",
        "        img, label = data\n",
        "        label_t1, label_t2 = torch.split(label, [15, 15], dim=1)\n",
        "        img = img.view(img.size(0), -1)\n",
        "        if use_gpu:\n",
        "            img = img.cuda()\n",
        "            label = label.cuda()\n",
        "        with torch.no_grad():\n",
        "          out1, out2 = model(img) \n",
        "          # estimate out with label\n",
        "          loss1 = criterion1(out1, label_t1)\n",
        "          loss2 = criterion1(out2, label_t2)\n",
        "          focalloss  = loss1 + loss2\n",
        "\n",
        "          mseloss1 = criterion2(out1, label_t1)\n",
        "          mseloss2 = criterion2(out2, label_t2)\n",
        "          mseloss = mseloss1 + mseloss2\n",
        "\n",
        "        eval_loss += focalloss.item()\n",
        "        eval_mse  += mseloss.item()\n",
        "        \n",
        "        # if OutSave = None: \n",
        "        #   OutSave = out\n",
        "        # else:\n",
        "        #   OutSave = np.concatenate(OutSave, out, dim = )\n",
        "        # _, pred = torch.max(out, 1)\n",
        "        # eval_acc += (pred == label).float().mean()\n",
        "    print(f'Test Loss on Focal: {eval_loss/len(test_loader):.6f}')\n",
        "    print(f'Test Loss on MSE: {eval_mse/len(test_loader):.6f}\\n')\n",
        "    EvalLossSave.append(eval_loss/len(test_loader))    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lum0iC-vwqT9",
        "outputId": "2b61246e-9631-4c0b-e14c-dc330e7a4765"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out1: 15, out2: 15\n",
            "(7296, 75)\n",
            "(7296, 15)\n",
            "(7296, 15)\n",
            "**********\n",
            "epoch 1\n",
            "Finish 1 epoch, Loss: 0.083924\n",
            "Test Loss on Focal: 0.013081\n",
            "Test Loss on MSE: 0.037413\n",
            "\n",
            "**********\n",
            "epoch 2\n",
            "Finish 2 epoch, Loss: 0.009952\n",
            "Test Loss on Focal: 0.008113\n",
            "Test Loss on MSE: 0.030219\n",
            "\n",
            "**********\n",
            "epoch 3\n",
            "Finish 3 epoch, Loss: 0.007616\n",
            "Test Loss on Focal: 0.007092\n",
            "Test Loss on MSE: 0.029485\n",
            "\n",
            "**********\n",
            "epoch 4\n",
            "Finish 4 epoch, Loss: 0.006969\n",
            "Test Loss on Focal: 0.006694\n",
            "Test Loss on MSE: 0.029293\n",
            "\n",
            "**********\n",
            "epoch 5\n",
            "Finish 5 epoch, Loss: 0.006720\n",
            "Test Loss on Focal: 0.006525\n",
            "Test Loss on MSE: 0.029294\n",
            "\n",
            "**********\n",
            "epoch 6\n",
            "Finish 6 epoch, Loss: 0.006567\n",
            "Test Loss on Focal: 0.006416\n",
            "Test Loss on MSE: 0.029262\n",
            "\n",
            "**********\n",
            "epoch 7\n",
            "Finish 7 epoch, Loss: 0.006493\n",
            "Test Loss on Focal: 0.006363\n",
            "Test Loss on MSE: 0.029252\n",
            "\n",
            "**********\n",
            "epoch 8\n",
            "Finish 8 epoch, Loss: 0.006448\n",
            "Test Loss on Focal: 0.006312\n",
            "Test Loss on MSE: 0.029291\n",
            "\n",
            "**********\n",
            "epoch 9\n",
            "Finish 9 epoch, Loss: 0.006390\n",
            "Test Loss on Focal: 0.006287\n",
            "Test Loss on MSE: 0.029278\n",
            "\n",
            "**********\n",
            "epoch 10\n",
            "Finish 10 epoch, Loss: 0.006371\n",
            "Test Loss on Focal: 0.006270\n",
            "Test Loss on MSE: 0.029238\n",
            "\n",
            "**********\n",
            "epoch 11\n",
            "Finish 11 epoch, Loss: 0.006369\n",
            "Test Loss on Focal: 0.006245\n",
            "Test Loss on MSE: 0.029292\n",
            "\n",
            "**********\n",
            "epoch 12\n",
            "Finish 12 epoch, Loss: 0.006340\n",
            "Test Loss on Focal: 0.006232\n",
            "Test Loss on MSE: 0.029300\n",
            "\n",
            "**********\n",
            "epoch 13\n",
            "Finish 13 epoch, Loss: 0.006345\n",
            "Test Loss on Focal: 0.006228\n",
            "Test Loss on MSE: 0.029266\n",
            "\n",
            "**********\n",
            "epoch 14\n",
            "Finish 14 epoch, Loss: 0.006334\n",
            "Test Loss on Focal: 0.006244\n",
            "Test Loss on MSE: 0.029264\n",
            "\n",
            "**********\n",
            "epoch 15\n",
            "Finish 15 epoch, Loss: 0.006313\n",
            "Test Loss on Focal: 0.006214\n",
            "Test Loss on MSE: 0.029274\n",
            "\n",
            "**********\n",
            "epoch 16\n",
            "Finish 16 epoch, Loss: 0.006331\n",
            "Test Loss on Focal: 0.006214\n",
            "Test Loss on MSE: 0.029272\n",
            "\n",
            "**********\n",
            "epoch 17\n",
            "Finish 17 epoch, Loss: 0.006314\n",
            "Test Loss on Focal: 0.006206\n",
            "Test Loss on MSE: 0.029246\n",
            "\n",
            "**********\n",
            "epoch 18\n",
            "Finish 18 epoch, Loss: 0.006344\n",
            "Test Loss on Focal: 0.006216\n",
            "Test Loss on MSE: 0.029297\n",
            "\n",
            "**********\n",
            "epoch 19\n",
            "Finish 19 epoch, Loss: 0.006338\n",
            "Test Loss on Focal: 0.006222\n",
            "Test Loss on MSE: 0.029376\n",
            "\n",
            "**********\n",
            "epoch 20\n",
            "Finish 20 epoch, Loss: 0.006314\n",
            "Test Loss on Focal: 0.006189\n",
            "Test Loss on MSE: 0.029238\n",
            "\n",
            "**********\n",
            "epoch 21\n",
            "Finish 21 epoch, Loss: 0.006307\n",
            "Test Loss on Focal: 0.006232\n",
            "Test Loss on MSE: 0.029335\n",
            "\n",
            "**********\n",
            "epoch 22\n",
            "Finish 22 epoch, Loss: 0.006290\n",
            "Test Loss on Focal: 0.006196\n",
            "Test Loss on MSE: 0.029269\n",
            "\n",
            "**********\n",
            "epoch 23\n",
            "Finish 23 epoch, Loss: 0.006330\n",
            "Test Loss on Focal: 0.006186\n",
            "Test Loss on MSE: 0.029250\n",
            "\n",
            "**********\n",
            "epoch 24\n",
            "Finish 24 epoch, Loss: 0.006294\n",
            "Test Loss on Focal: 0.006171\n",
            "Test Loss on MSE: 0.029284\n",
            "\n",
            "**********\n",
            "epoch 25\n",
            "Finish 25 epoch, Loss: 0.006289\n",
            "Test Loss on Focal: 0.006213\n",
            "Test Loss on MSE: 0.029371\n",
            "\n",
            "**********\n",
            "epoch 26\n",
            "Finish 26 epoch, Loss: 0.006293\n",
            "Test Loss on Focal: 0.006181\n",
            "Test Loss on MSE: 0.029254\n",
            "\n",
            "**********\n",
            "epoch 27\n",
            "Finish 27 epoch, Loss: 0.006303\n",
            "Test Loss on Focal: 0.006232\n",
            "Test Loss on MSE: 0.029394\n",
            "\n",
            "**********\n",
            "epoch 28\n",
            "Finish 28 epoch, Loss: 0.006290\n",
            "Test Loss on Focal: 0.006193\n",
            "Test Loss on MSE: 0.029248\n",
            "\n",
            "**********\n",
            "epoch 29\n",
            "Finish 29 epoch, Loss: 0.006298\n",
            "Test Loss on Focal: 0.006188\n",
            "Test Loss on MSE: 0.029338\n",
            "\n",
            "**********\n",
            "epoch 30\n",
            "Finish 30 epoch, Loss: 0.006293\n",
            "Test Loss on Focal: 0.006178\n",
            "Test Loss on MSE: 0.029286\n",
            "\n",
            "**********\n",
            "epoch 31\n",
            "Finish 31 epoch, Loss: 0.006288\n",
            "Test Loss on Focal: 0.006203\n",
            "Test Loss on MSE: 0.029358\n",
            "\n",
            "**********\n",
            "epoch 32\n",
            "Finish 32 epoch, Loss: 0.006285\n",
            "Test Loss on Focal: 0.006221\n",
            "Test Loss on MSE: 0.029255\n",
            "\n",
            "**********\n",
            "epoch 33\n",
            "Finish 33 epoch, Loss: 0.006294\n",
            "Test Loss on Focal: 0.006214\n",
            "Test Loss on MSE: 0.029246\n",
            "\n",
            "**********\n",
            "epoch 34\n",
            "Finish 34 epoch, Loss: 0.006305\n",
            "Test Loss on Focal: 0.006236\n",
            "Test Loss on MSE: 0.029315\n",
            "\n",
            "**********\n",
            "epoch 35\n",
            "Finish 35 epoch, Loss: 0.006289\n",
            "Test Loss on Focal: 0.006210\n",
            "Test Loss on MSE: 0.029294\n",
            "\n",
            "**********\n",
            "epoch 36\n",
            "Finish 36 epoch, Loss: 0.006286\n",
            "Test Loss on Focal: 0.006213\n",
            "Test Loss on MSE: 0.029301\n",
            "\n",
            "**********\n",
            "epoch 37\n",
            "Finish 37 epoch, Loss: 0.006294\n",
            "Test Loss on Focal: 0.006191\n",
            "Test Loss on MSE: 0.029273\n",
            "\n",
            "**********\n",
            "epoch 38\n",
            "Finish 38 epoch, Loss: 0.006284\n",
            "Test Loss on Focal: 0.006205\n",
            "Test Loss on MSE: 0.029326\n",
            "\n",
            "**********\n",
            "epoch 39\n",
            "Finish 39 epoch, Loss: 0.006289\n",
            "Test Loss on Focal: 0.006197\n",
            "Test Loss on MSE: 0.029407\n",
            "\n",
            "**********\n",
            "epoch 40\n",
            "Finish 40 epoch, Loss: 0.006307\n",
            "Test Loss on Focal: 0.006179\n",
            "Test Loss on MSE: 0.029270\n",
            "\n",
            "**********\n",
            "epoch 41\n",
            "Finish 41 epoch, Loss: 0.006290\n",
            "Test Loss on Focal: 0.006207\n",
            "Test Loss on MSE: 0.029402\n",
            "\n",
            "**********\n",
            "epoch 42\n",
            "Finish 42 epoch, Loss: 0.006303\n",
            "Test Loss on Focal: 0.006257\n",
            "Test Loss on MSE: 0.029395\n",
            "\n",
            "**********\n",
            "epoch 43\n",
            "Finish 43 epoch, Loss: 0.006297\n",
            "Test Loss on Focal: 0.006200\n",
            "Test Loss on MSE: 0.029299\n",
            "\n",
            "**********\n",
            "epoch 44\n",
            "Finish 44 epoch, Loss: 0.006291\n",
            "Test Loss on Focal: 0.006222\n",
            "Test Loss on MSE: 0.029292\n",
            "\n",
            "**********\n",
            "epoch 45\n",
            "Finish 45 epoch, Loss: 0.006286\n",
            "Test Loss on Focal: 0.006181\n",
            "Test Loss on MSE: 0.029249\n",
            "\n",
            "**********\n",
            "epoch 46\n",
            "Finish 46 epoch, Loss: 0.006299\n",
            "Test Loss on Focal: 0.006180\n",
            "Test Loss on MSE: 0.029282\n",
            "\n",
            "**********\n",
            "epoch 47\n",
            "Finish 47 epoch, Loss: 0.006276\n",
            "Test Loss on Focal: 0.006188\n",
            "Test Loss on MSE: 0.029222\n",
            "\n",
            "**********\n",
            "epoch 48\n",
            "Finish 48 epoch, Loss: 0.006299\n",
            "Test Loss on Focal: 0.006198\n",
            "Test Loss on MSE: 0.029262\n",
            "\n",
            "**********\n",
            "epoch 49\n",
            "Finish 49 epoch, Loss: 0.006281\n",
            "Test Loss on Focal: 0.006209\n",
            "Test Loss on MSE: 0.029277\n",
            "\n",
            "**********\n",
            "epoch 50\n",
            "Finish 50 epoch, Loss: 0.006295\n",
            "Test Loss on Focal: 0.006188\n",
            "Test Loss on MSE: 0.029325\n",
            "\n",
            "**********\n",
            "epoch 51\n",
            "Finish 51 epoch, Loss: 0.006288\n",
            "Test Loss on Focal: 0.006193\n",
            "Test Loss on MSE: 0.029269\n",
            "\n",
            "**********\n",
            "epoch 52\n",
            "Finish 52 epoch, Loss: 0.006284\n",
            "Test Loss on Focal: 0.006185\n",
            "Test Loss on MSE: 0.029243\n",
            "\n",
            "**********\n",
            "epoch 53\n",
            "Finish 53 epoch, Loss: 0.006280\n",
            "Test Loss on Focal: 0.006200\n",
            "Test Loss on MSE: 0.029260\n",
            "\n",
            "**********\n",
            "epoch 54\n",
            "Finish 54 epoch, Loss: 0.006302\n",
            "Test Loss on Focal: 0.006187\n",
            "Test Loss on MSE: 0.029299\n",
            "\n",
            "**********\n",
            "epoch 55\n",
            "Finish 55 epoch, Loss: 0.006292\n",
            "Test Loss on Focal: 0.006207\n",
            "Test Loss on MSE: 0.029328\n",
            "\n",
            "**********\n",
            "epoch 56\n",
            "Finish 56 epoch, Loss: 0.006296\n",
            "Test Loss on Focal: 0.006194\n",
            "Test Loss on MSE: 0.029241\n",
            "\n",
            "**********\n",
            "epoch 57\n",
            "Finish 57 epoch, Loss: 0.006282\n",
            "Test Loss on Focal: 0.006198\n",
            "Test Loss on MSE: 0.029247\n",
            "\n",
            "**********\n",
            "epoch 58\n",
            "Finish 58 epoch, Loss: 0.006294\n",
            "Test Loss on Focal: 0.006177\n",
            "Test Loss on MSE: 0.029244\n",
            "\n",
            "**********\n",
            "epoch 59\n",
            "Finish 59 epoch, Loss: 0.006285\n",
            "Test Loss on Focal: 0.006194\n",
            "Test Loss on MSE: 0.029315\n",
            "\n",
            "**********\n",
            "epoch 60\n",
            "Finish 60 epoch, Loss: 0.006275\n",
            "Test Loss on Focal: 0.006201\n",
            "Test Loss on MSE: 0.029339\n",
            "\n",
            "**********\n",
            "epoch 61\n",
            "Finish 61 epoch, Loss: 0.006295\n",
            "Test Loss on Focal: 0.006193\n",
            "Test Loss on MSE: 0.029318\n",
            "\n",
            "**********\n",
            "epoch 62\n",
            "Finish 62 epoch, Loss: 0.006305\n",
            "Test Loss on Focal: 0.006214\n",
            "Test Loss on MSE: 0.029336\n",
            "\n",
            "**********\n",
            "epoch 63\n",
            "Finish 63 epoch, Loss: 0.006294\n",
            "Test Loss on Focal: 0.006189\n",
            "Test Loss on MSE: 0.029309\n",
            "\n",
            "**********\n",
            "epoch 64\n",
            "Finish 64 epoch, Loss: 0.006300\n",
            "Test Loss on Focal: 0.006179\n",
            "Test Loss on MSE: 0.029277\n",
            "\n",
            "**********\n",
            "epoch 65\n",
            "Finish 65 epoch, Loss: 0.006313\n",
            "Test Loss on Focal: 0.006199\n",
            "Test Loss on MSE: 0.029306\n",
            "\n",
            "**********\n",
            "epoch 66\n",
            "Finish 66 epoch, Loss: 0.006281\n",
            "Test Loss on Focal: 0.006174\n",
            "Test Loss on MSE: 0.029248\n",
            "\n",
            "**********\n",
            "epoch 67\n",
            "Finish 67 epoch, Loss: 0.006297\n",
            "Test Loss on Focal: 0.006176\n",
            "Test Loss on MSE: 0.029251\n",
            "\n",
            "**********\n",
            "epoch 68\n",
            "Finish 68 epoch, Loss: 0.006281\n",
            "Test Loss on Focal: 0.006194\n",
            "Test Loss on MSE: 0.029312\n",
            "\n",
            "**********\n",
            "epoch 69\n",
            "Finish 69 epoch, Loss: 0.006291\n",
            "Test Loss on Focal: 0.006221\n",
            "Test Loss on MSE: 0.029247\n",
            "\n",
            "**********\n",
            "epoch 70\n",
            "Finish 70 epoch, Loss: 0.006290\n",
            "Test Loss on Focal: 0.006182\n",
            "Test Loss on MSE: 0.029302\n",
            "\n",
            "**********\n",
            "epoch 71\n",
            "Finish 71 epoch, Loss: 0.006282\n",
            "Test Loss on Focal: 0.006203\n",
            "Test Loss on MSE: 0.029341\n",
            "\n",
            "**********\n",
            "epoch 72\n",
            "Finish 72 epoch, Loss: 0.006293\n",
            "Test Loss on Focal: 0.006204\n",
            "Test Loss on MSE: 0.029488\n",
            "\n",
            "**********\n",
            "epoch 73\n",
            "Finish 73 epoch, Loss: 0.006291\n",
            "Test Loss on Focal: 0.006231\n",
            "Test Loss on MSE: 0.029372\n",
            "\n",
            "**********\n",
            "epoch 74\n",
            "Finish 74 epoch, Loss: 0.006292\n",
            "Test Loss on Focal: 0.006214\n",
            "Test Loss on MSE: 0.029337\n",
            "\n",
            "**********\n",
            "epoch 75\n",
            "Finish 75 epoch, Loss: 0.006308\n",
            "Test Loss on Focal: 0.006182\n",
            "Test Loss on MSE: 0.029264\n",
            "\n",
            "**********\n",
            "epoch 76\n",
            "Finish 76 epoch, Loss: 0.006295\n",
            "Test Loss on Focal: 0.006185\n",
            "Test Loss on MSE: 0.029245\n",
            "\n",
            "**********\n",
            "epoch 77\n",
            "Finish 77 epoch, Loss: 0.006292\n",
            "Test Loss on Focal: 0.006222\n",
            "Test Loss on MSE: 0.029328\n",
            "\n",
            "**********\n",
            "epoch 78\n",
            "Finish 78 epoch, Loss: 0.006288\n",
            "Test Loss on Focal: 0.006199\n",
            "Test Loss on MSE: 0.029265\n",
            "\n",
            "**********\n",
            "epoch 79\n",
            "Finish 79 epoch, Loss: 0.006284\n",
            "Test Loss on Focal: 0.006190\n",
            "Test Loss on MSE: 0.029247\n",
            "\n",
            "**********\n",
            "epoch 80\n",
            "Finish 80 epoch, Loss: 0.006297\n",
            "Test Loss on Focal: 0.006182\n",
            "Test Loss on MSE: 0.029308\n",
            "\n",
            "**********\n",
            "epoch 81\n",
            "Finish 81 epoch, Loss: 0.006303\n",
            "Test Loss on Focal: 0.006215\n",
            "Test Loss on MSE: 0.029390\n",
            "\n",
            "**********\n",
            "epoch 82\n",
            "Finish 82 epoch, Loss: 0.006290\n",
            "Test Loss on Focal: 0.006221\n",
            "Test Loss on MSE: 0.029464\n",
            "\n",
            "**********\n",
            "epoch 83\n",
            "Finish 83 epoch, Loss: 0.006284\n",
            "Test Loss on Focal: 0.006197\n",
            "Test Loss on MSE: 0.029296\n",
            "\n",
            "**********\n",
            "epoch 84\n",
            "Finish 84 epoch, Loss: 0.006284\n",
            "Test Loss on Focal: 0.006176\n",
            "Test Loss on MSE: 0.029264\n",
            "\n",
            "**********\n",
            "epoch 85\n",
            "Finish 85 epoch, Loss: 0.006302\n",
            "Test Loss on Focal: 0.006186\n",
            "Test Loss on MSE: 0.029288\n",
            "\n",
            "**********\n",
            "epoch 86\n",
            "Finish 86 epoch, Loss: 0.006296\n",
            "Test Loss on Focal: 0.006208\n",
            "Test Loss on MSE: 0.029448\n",
            "\n",
            "**********\n",
            "epoch 87\n",
            "Finish 87 epoch, Loss: 0.006318\n",
            "Test Loss on Focal: 0.006180\n",
            "Test Loss on MSE: 0.029303\n",
            "\n",
            "**********\n",
            "epoch 88\n",
            "Finish 88 epoch, Loss: 0.006293\n",
            "Test Loss on Focal: 0.006182\n",
            "Test Loss on MSE: 0.029305\n",
            "\n",
            "**********\n",
            "epoch 89\n",
            "Finish 89 epoch, Loss: 0.006288\n",
            "Test Loss on Focal: 0.006195\n",
            "Test Loss on MSE: 0.029277\n",
            "\n",
            "**********\n",
            "epoch 90\n",
            "Finish 90 epoch, Loss: 0.006305\n",
            "Test Loss on Focal: 0.006170\n",
            "Test Loss on MSE: 0.029274\n",
            "\n",
            "**********\n",
            "epoch 91\n",
            "Finish 91 epoch, Loss: 0.006316\n",
            "Test Loss on Focal: 0.006176\n",
            "Test Loss on MSE: 0.029265\n",
            "\n",
            "**********\n",
            "epoch 92\n",
            "Finish 92 epoch, Loss: 0.006274\n",
            "Test Loss on Focal: 0.006175\n",
            "Test Loss on MSE: 0.029295\n",
            "\n",
            "**********\n",
            "epoch 93\n",
            "Finish 93 epoch, Loss: 0.006278\n",
            "Test Loss on Focal: 0.006222\n",
            "Test Loss on MSE: 0.029311\n",
            "\n",
            "**********\n",
            "epoch 94\n",
            "Finish 94 epoch, Loss: 0.006293\n",
            "Test Loss on Focal: 0.006185\n",
            "Test Loss on MSE: 0.029309\n",
            "\n",
            "**********\n",
            "epoch 95\n",
            "Finish 95 epoch, Loss: 0.006294\n",
            "Test Loss on Focal: 0.006213\n",
            "Test Loss on MSE: 0.029274\n",
            "\n",
            "**********\n",
            "epoch 96\n",
            "Finish 96 epoch, Loss: 0.006309\n",
            "Test Loss on Focal: 0.006199\n",
            "Test Loss on MSE: 0.029265\n",
            "\n",
            "**********\n",
            "epoch 97\n",
            "Finish 97 epoch, Loss: 0.006297\n",
            "Test Loss on Focal: 0.006203\n",
            "Test Loss on MSE: 0.029282\n",
            "\n",
            "**********\n",
            "epoch 98\n",
            "Finish 98 epoch, Loss: 0.006302\n",
            "Test Loss on Focal: 0.006197\n",
            "Test Loss on MSE: 0.029306\n",
            "\n",
            "**********\n",
            "epoch 99\n",
            "Finish 99 epoch, Loss: 0.006300\n",
            "Test Loss on Focal: 0.006198\n",
            "Test Loss on MSE: 0.029266\n",
            "\n",
            "**********\n",
            "epoch 100\n",
            "Finish 100 epoch, Loss: 0.006278\n",
            "Test Loss on Focal: 0.006200\n",
            "Test Loss on MSE: 0.029311\n",
            "\n"
          ]
        }
      ]
    }
  ]
}